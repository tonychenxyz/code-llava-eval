#!/usr/bin/env python3

"""
Minimal evaluation helper for datasets generated by prepare_dataset.py.

Given a reference JSONL (with `index` and `outputs`) and a prediction JSONL
that mirrors those rows but with a `pred` field, this script computes a simple
string-match accuracy (case-insensitive substring check for every reference).
"""

from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple

def load_jsonl(path: Path) -> List[dict]:
    rows = []
    with path.open("r", encoding="utf-8") as handle:
        for line in handle:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows


def normalize(text: str) -> str:
    return " ".join(text.strip().lower().split())


def extract_digits(text: str) -> List[str]:
    return re.findall(r"\d+", text)


def score_prediction(pred: str, references: List[str], task_type: str = "") -> Tuple[bool, float]:
    if not references:
        return False, 0.0
    pred_norm = normalize(pred)
    if task_type == "niah":
        hits = 0
        for ref in references:
            digits_ref = extract_digits(ref)
            if digits_ref:
                # Only compare the first digit string (e.g., "derpkp: 590785" -> "590785")
                target = digits_ref[0]
                if target and target in pred:
                    hits += 1
            elif normalize(ref) in pred_norm:
                # Fallback for malformed references without digits.
                hits += 1
        full_match = hits == len(references)
        return full_match, hits / max(1, len(references))
    else:
        hits = 0
        for ref in references:
            if normalize(ref) in pred_norm:
                hits += 1
        full_match = hits == len(references)
        return full_match, hits / max(1, len(references))


def evaluate(ref_path: Path, pred_path: Path, verbose: int = 0, task_type: str = "") -> None:
    ref_rows = load_jsonl(ref_path)
    pred_rows = load_jsonl(pred_path)

    ref_by_index: Dict[int, List[str]] = {
        row["index"]: row.get("outputs", []) for row in ref_rows
    }

    total = 0
    exact = 0
    partial_scores: List[float] = []

    for row in pred_rows:
        idx = row["index"]
        pred = row.get("pred", "")
        refs = ref_by_index.get(idx, [])
        full, partial = score_prediction(pred, refs, task_type=task_type)
        total += 1
        partial_scores.append(partial)
        if full:
            exact += 1
        elif verbose > 0 and verbose >= total:
            print("=" * 40)
            print(f"Index      : {idx}")
            print(f"Reference  : {refs}")
            print(f"Prediction : {pred}")
            print(f"Match ratio: {partial:.2f}")

    accuracy = exact / total if total else 0.0
    avg_partial = sum(partial_scores) / total if total else 0.0

    print(f"Samples evaluated : {total}")
    print(f"Exact-match score : {accuracy * 100:.2f}")
    print(f"Average match rate: {avg_partial * 100:.2f}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Evaluate predictions produced on prepare_dataset.py JSONL files."
    )
    parser.add_argument("--ref", type=Path, required=True, help="Reference JSONL path.")
    parser.add_argument("--pred", type=Path, required=True, help="Prediction JSONL path.")
    parser.add_argument(
        "--verbose",
        type=int,
        default=0,
        help="Print up to N mismatched examples for inspection.",
    )
    parser.add_argument("--task-type", type=str, default="", help="Optional task alias (e.g., 'niah').")
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    evaluate(args.ref, args.pred, verbose=args.verbose, task_type=args.task_type)


if __name__ == "__main__":
    main()
